{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<font color=black><br>\n",
    "\n",
    "**What?** Optimising Pandas - reduce memory footprint\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- Less than 100 MB Pandas is OK\n",
    "- More than >100 Mb to xx GBs Pandas starts to suffer\n",
    "- In this second case people start contemplating Spark which can handle several GBs or even TBs.\n",
    "- However, Spark lack a rich feature sets for high quality data cleaning, exploration, and analysis.\n",
    "- **Bottom line?** For For medium-sized data, we're better off trying to get more out of pandas, rather than switching to a different tool.\n",
    "- Here, we'll show how by selecting the right data tyoes in the column can reduce the meomory footprint.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>beer_servings</th>\n",
       "      <th>spirit_servings</th>\n",
       "      <th>wine_servings</th>\n",
       "      <th>total_litres_of_pure_alcohol</th>\n",
       "      <th>continent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>89</td>\n",
       "      <td>132</td>\n",
       "      <td>54</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>245</td>\n",
       "      <td>138</td>\n",
       "      <td>312</td>\n",
       "      <td>12.4</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angola</td>\n",
       "      <td>217</td>\n",
       "      <td>57</td>\n",
       "      <td>45</td>\n",
       "      <td>5.9</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  beer_servings  spirit_servings  wine_servings  \\\n",
       "0  Afghanistan              0                0              0   \n",
       "1      Albania             89              132             54   \n",
       "2      Algeria             25                0             14   \n",
       "3      Andorra            245              138            312   \n",
       "4       Angola            217               57             45   \n",
       "\n",
       "   total_litres_of_pure_alcohol continent  \n",
       "0                           0.0      Asia  \n",
       "1                           4.9    Europe  \n",
       "2                           0.7    Africa  \n",
       "3                          12.4    Europe  \n",
       "4                           5.9    Africa  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drinks = pd.read_csv('http://bit.ly/drinksbycountry')\n",
    "drinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- We'll first look at the memory usage of each column, because we're interested in accuracy\n",
    "- We'll set the argument deep to True to get an accurate number.\n",
    "  \n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                             128\n",
       "country                         12588\n",
       "beer_servings                    1544\n",
       "spirit_servings                  1544\n",
       "wine_servings                    1544\n",
       "total_litres_of_pure_alcohol     1544\n",
       "continent                       12332\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drinks.memory_usage(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 193 entries, 0 to 192\n",
      "Data columns (total 6 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   country                       193 non-null    object \n",
      " 1   beer_servings                 193 non-null    int64  \n",
      " 2   spirit_servings               193 non-null    int64  \n",
      " 3   wine_servings                 193 non-null    int64  \n",
      " 4   total_litres_of_pure_alcohol  193 non-null    float64\n",
      " 5   continent                     193 non-null    object \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 30.5 KB\n"
     ]
    }
   ],
   "source": [
    "# This is another option\n",
    "drinks.info(memory_usage = 'deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- Pandas groups the columns into blocks of values of the same type, because each data type is stored separately.\n",
    "- Weâ€™re going to examine the memory usage by each data type.\n",
    "- Immediately we can see that **most of the memory** is used by our object columns. \n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average memory usage for float columns: 836.00 B\n",
      "Average memory usage for int columns: 1190.00 B\n",
      "Average memory usage for object columns: 8349.33 B\n"
     ]
    }
   ],
   "source": [
    "for dtype in ('float', 'int', 'object'):\n",
    "    selected_dtype = drinks.select_dtypes(include = [dtype])\n",
    "    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n",
    "    # we can do / 1024 ** 2 to convert bytes to megabytes\n",
    "    print(\"Average memory usage for {} columns: {:03.2f} B\".format(dtype, mean_usage_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Numeric Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- For blocks representing numeric values like integers and floats, pandas combines the columns and stores them as a NumPy ndarray. The NumPy ndarray is built around a C array, and the values are stored in a contiguous block of memory. This storage model consumes less space and allows us to access the values themselves quickly.\n",
    "- Many types in pandas have multiple subtypes that **can use fewer bytes** to represent each value and that is something we can used to save the memory footprint.\n",
    "- An int8 value uses 1 byte (or 8 bits) to store a value, and can represent 256 values (2^8) in binary. This means that we can use this subtype to represent values ranging from -128 to 127 (including 0). And uint8, which is unsigned int, means we can only have positive values for this type, thus we can represent 256 values ranging from 0 to 255.\n",
    "- To be able to use it we just need the min/max of our data via **numpy.iinfo** <br>\n",
    "\n",
    "|memory usage|\tfloat|\tint\t|uint\t|datetime  |bool  |\n",
    "|------------|-------|------|-------|----------|------|\n",
    "|1 bytes\t | \t     | int8 |uint8  |\t \t   |bool  |\n",
    "|2 bytes\t |float16| int16|uint16 |\t \t   |      |\n",
    "|4 bytes\t |float32| int32|uint32 |\t \t   |      |\n",
    "|8 bytes\t |float64| int64|uint64 |datetime64|\t  |\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine parameters for uint8\n",
      "---------------------------------------------------------------\n",
      "min = 0\n",
      "max = 255\n",
      "---------------------------------------------------------------\n",
      "\n",
      "Machine parameters for int8\n",
      "---------------------------------------------------------------\n",
      "min = -128\n",
      "max = 127\n",
      "---------------------------------------------------------------\n",
      "\n",
      "Machine parameters for int16\n",
      "---------------------------------------------------------------\n",
      "min = -32768\n",
      "max = 32767\n",
      "---------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "int_types = ['uint8', 'int8', 'int16']\n",
    "for int_type in int_types:\n",
    "    print(np.iinfo(int_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- Weâ€™ll use **DataFrame.select_dtypes** to select only the integer columns.\n",
    "- We'll use **pd.to_numeric()** to downcast our numeric types. \n",
    "- Then weâ€™ll optimize the types and compare the memory usage.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    \"\"\"memory usage of a pandas DataFrame or Series\"\"\"\n",
    "    # we assume if not a DataFrame it's a Series\n",
    "    if isinstance(pandas_obj, pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else:\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "\n",
    "    return '{:03.2f} B'.format(usage_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4760.00 B\n",
      "1286.00 B\n"
     ]
    }
   ],
   "source": [
    "drinks_int = drinks.select_dtypes(include=['int'])\n",
    "converted_int = drinks_int.apply(pd.to_numeric, downcast='unsigned')\n",
    "\n",
    "print(mem_usage(drinks_int))\n",
    "print(mem_usage(converted_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1672.00 B\n",
      "900.00 B\n"
     ]
    }
   ],
   "source": [
    "# Lets do the same thing with our float columns.\n",
    "drinks_float = drinks.select_dtypes(include=['float'])\n",
    "converted_float = drinks_float.apply(pd.to_numeric, downcast='float')\n",
    "\n",
    "print(mem_usage(drinks_float))\n",
    "print(mem_usage(converted_float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing object types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- The object type represents values using Python string objects, partly due to the lack of support for missing string values in NumPy. Because Python is a high-level, interpreted language, it doesn't have fine grained-control over how values in memory are stored.\n",
    "- To overcome this problem, Pandas introduced Categoricals in version 0.15.\n",
    "- Since the country and continent columns are strings, they are represented as object types in pandas. \n",
    "- We want to store the continent column as integers to reduce the memory required to store them by converting it to categorical type. \n",
    "- To apply this conversion, we simply have to convert the column type to category using the .astype method.\n",
    "- The comparison shows that by converting the continent column to integers we're being more space-efficient. Apart from that it can actually speed up laters operations, e.g. sorting, groupby as we're storing the strings as compactly as integers. \n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Asia\n",
      "1    Europe\n",
      "2    Africa\n",
      "3    Europe\n",
      "4    Africa\n",
      "Name: continent, dtype: object\n",
      "0      Asia\n",
      "1    Europe\n",
      "2    Africa\n",
      "3    Europe\n",
      "4    Africa\n",
      "Name: continent, dtype: category\n",
      "Categories (6, object): ['Africa', 'Asia', 'Europe', 'North America', 'Oceania', 'South America']\n"
     ]
    }
   ],
   "source": [
    "# convert and print the memory usage\n",
    "continent_col = 'continent'\n",
    "continent = drinks[continent_col]\n",
    "continent_cat = drinks[continent_col].astype('category')\n",
    "print(continent.head())\n",
    "print(continent_cat.head())\n",
    "# drinks.memory_usage(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  12460.00 B\n",
      "categorical:  884.00 B\n"
     ]
    }
   ],
   "source": [
    "# Lastly, letâ€™s look at the memory usage for this column before and after converting to the category type.\n",
    "print('original: ', mem_usage(continent))\n",
    "print('categorical: ', mem_usage(continent_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- Let's apply this notion again to the country column.\n",
    "- This time, the memory usage for the country column is now larger.\n",
    "- The **reason** is that the country column's value is unique.\n",
    "- So **do not use** of the cardinality (distinguished values) is high.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  12716.00 B\n",
      "categorical:  18222.00 B\n"
     ]
    }
   ],
   "source": [
    "country_col = 'country'\n",
    "country = drinks[country_col]\n",
    "country_cat = drinks[country_col].astype('category')\n",
    "print('original: ', mem_usage(country))\n",
    "print('categorical: ', mem_usage(country_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Types While Reading the Data In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- **How can we apply this if we can't even create the dataframe in the first place?**\n",
    "- Fortunately, we can specify the optimal column types **when reading** the data. \n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                             128\n",
       "country                         12588\n",
       "beer_servings                     772\n",
       "spirit_servings                   772\n",
       "wine_servings                     772\n",
       "total_litres_of_pure_alcohol      772\n",
       "continent                         756\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_types = {'beer_servings': 'uint32',\n",
    "             'continent': 'category',\n",
    "             'country': 'object',\n",
    "             'spirit_servings': 'uint32',\n",
    "             'total_litres_of_pure_alcohol': 'float32',\n",
    "             'wine_servings': 'uint32'}\n",
    "\n",
    "df_drinks = pd.read_csv('http://bit.ly/drinksbycountry', dtype=col_types)\n",
    "df_drinks.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or instead of manually specifying the type, we can leverage a function to automatically perform the memory reduction for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to automatically perform downcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, blacklist_cols=None):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of the dataframe and downcast the\n",
    "    data type to reduce memory usage.\n",
    "\n",
    "    The logic is numeric type will be downcast to the smallest possible\n",
    "    numeric type. e.g. if an int column's value ranges from 1 - 8, then it\n",
    "    fits into an int8 type, and will be downcast to int8.\n",
    "    And object type will be converted to categorical type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe prior the memory reduction.\n",
    "\n",
    "    blacklist_cols : collection[str], e.g. list[str], set[str]\n",
    "        A collection of column names that won't go through the memory\n",
    "        reduction process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe post memory reduction.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "    \"\"\"\n",
    "    start_mem = compute_df_total_mem(df)\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    blacklist_cols = blacklist_cols if blacklist_cols else set()\n",
    "    for col in df.columns:\n",
    "        if col in blacklist_cols:\n",
    "            continue\n",
    "\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = compute_df_total_mem(df)\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(\n",
    "        100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_df_total_mem(df):\n",
    "    \"\"\"Returns a dataframe's total memory usage in MB.\"\"\"\n",
    "    return df.memory_usage(deep=True).sum() / 1024 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.03 MB\n",
      "Memory usage after optimization is: 0.01 MB\n",
      "Decreased by 51.9%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index                             128\n",
       "country                         12588\n",
       "beer_servings                     386\n",
       "spirit_servings                   386\n",
       "wine_servings                     386\n",
       "total_litres_of_pure_alcohol      386\n",
       "continent                         756\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drinks = pd.read_csv('http://bit.ly/drinksbycountry')\n",
    "df_drinks = reduce_mem_usage(df_drinks, blacklist_cols=['country'])\n",
    "df_drinks.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<font color=black><br>\n",
    "\n",
    "- Less than 100 MB Pandas is OK\n",
    "- More than >100 Mb to xx GBs Pandas starts to suffer\n",
    "- In this second case people start contemplating Spark which can handle several GBs or even TBs.\n",
    "- However, Spark lack a rich feature sets for high quality data cleaning, exploration, and analysis.\n",
    "- **Bottom line?** For For medium-sized data, we're better off trying to get more out of pandas, rather than switching to a different tool.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [This notebook](http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/python/pandas/pandas.ipynb)\n",
    "- [Code to automatically reduced the memory footprint](https://www.kaggle.com/gemartin/load-data-reduce-memory-usage)\n",
    "- [Blog: Pandas Categoricals](https://www.continuum.io/content/pandas-categoricals)\n",
    "- [Blog: Using pandas with large data](https://www.dataquest.io/blog/pandas-big-data/)\n",
    "- [Youtube: How do I make my pandas DataFrame smaller and faster?](https://www.youtube.com/watch?v=wDYDYGyN_cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
